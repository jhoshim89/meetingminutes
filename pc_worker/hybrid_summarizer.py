"""
Hybrid Summarizer - í†µí•© íšŒì˜ ìš”ì•½ê¸°
====================================
êµ¬ì¡°í™”/ë¹„êµ¬ì¡°í™” íšŒì˜ ëª¨ë‘ ì²˜ë¦¬í•˜ëŠ” ë‹¨ì¼ ìš”ì•½ê¸°

ì¶œë ¥:
- ì£¼ìš” ì£¼ì œ + ë‹¤ìŒ í•  ì¼ (ìì—°ìŠ¤ëŸ¬ìš´ ìš”ì•½)
- ì•ˆê±´ë³„ ìƒì„¸ (í˜„í™©/ë…¼ì˜/ê²°ì˜ ë“± ìë™ ë¶„ë¥˜)
- ë‹¨ë½ë³„ íƒ€ì„ë¼ì¸ ìš”ì•½
"""

import json
import re
import subprocess
import time
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional, Any
from dataclasses import dataclass, field

from summarizer_utils import (
    call_ollama, chunk_transcript, parse_bullet_list,
    infer_category, CATEGORIES, DEFAULT_MODEL, OLLAMA_URL,
    ensure_ollama_ready, OllamaConnectionError, OllamaEmptyResponseError,
    logger
)


@dataclass
class HybridSummary:
    """í†µí•© ìš”ì•½ ê²°ê³¼"""
    main_topics: List[str] = field(default_factory=list)
    action_items: List[str] = field(default_factory=list)
    timeline_summaries: List[Dict] = field(default_factory=list)
    agenda_items: List[Dict] = field(default_factory=list)
    raw_text: str = ""
    processing_time: float = 0.0

    @property
    def summary(self) -> str:
        """MeetingSummary í˜¸í™˜: main_topicsë¥¼ ìš”ì•½ë¬¸ìœ¼ë¡œ ë³€í™˜"""
        if self.main_topics:
            return " ".join(self.main_topics)
        return self.raw_text[:500] if self.raw_text else "(ìš”ì•½ ì—†ìŒ)"

    @property
    def key_points(self) -> List[str]:
        """MeetingSummary í˜¸í™˜: main_topics ë°˜í™˜"""
        return self.main_topics

    @property
    def topics(self) -> List[str]:
        """MeetingSummary í˜¸í™˜: main_topics ë°˜í™˜"""
        return self.main_topics

    @property
    def categories(self) -> List[str]:
        """MeetingSummary í˜¸í™˜: ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
        return []


class SummaryValidationError(Exception):
    """ìš”ì•½ ê²°ê³¼ ê²€ì¦ ì‹¤íŒ¨"""
    pass


class HybridSummarizer:
    """í†µí•© íšŒì˜ ìš”ì•½ê¸°"""

    def __init__(
        self,
        model: str = DEFAULT_MODEL,
        ollama_url: str = OLLAMA_URL,
        check_health_on_init: bool = True,
        strict_validation: bool = True
    ):
        """
        Args:
            model: Ollama ëª¨ë¸ëª…
            ollama_url: Ollama ì„œë²„ URL
            check_health_on_init: ì´ˆê¸°í™” ì‹œ ì„œë²„ í—¬ìŠ¤ì²´í¬ ìˆ˜í–‰ ì—¬ë¶€
            strict_validation: ì—„ê²©í•œ ê²°ê³¼ ê²€ì¦ (ë¹ˆ ê²°ê³¼ ì‹œ ì˜ˆì™¸ ë°œìƒ)
        """
        self.model = model
        self.ollama_url = ollama_url
        self.strict_validation = strict_validation

        if check_health_on_init:
            ensure_ollama_ready(ollama_url, model)

    def _call_llm(self, prompt: str, temperature: float = 0.3) -> str:
        """LLM í˜¸ì¶œ ë˜í¼"""
        return call_ollama(
            prompt, self.model, self.ollama_url, temperature,
            raise_on_empty=self.strict_validation
        )

    def _summarize_chunk(self, time_range: str, chunk: str) -> Dict:
        """ì²­í¬ë¥¼ í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ìœ¼ë¡œ ìš”ì•½"""
        prompt = f"""ë‹¤ìŒ íšŒì˜ ë‚´ìš©ì„ ë¶„ì„í•˜ì„¸ìš”.

[íšŒì˜ ë‚´ìš©]
{chunk}

[ì¶œë ¥ í˜•ì‹]
ì œëª©: (í•µì‹¬ ì£¼ì œ 5-15ì)
ìš”ì•½: (ì „ì²´ ë‚´ìš©ì„ 2-3ë¬¸ì¥ìœ¼ë¡œ)
í¬ì¸íŠ¸:
- í¬ì¸íŠ¸1
- í¬ì¸íŠ¸2
- í¬ì¸íŠ¸3

ìœ„ í˜•ì‹ìœ¼ë¡œë§Œ ì‘ì„±í•˜ì„¸ìš”:"""

        response = self._call_llm(prompt)

        # íŒŒì‹±
        lines = response.strip().split('\n')
        title = ""
        summary = ""
        points = []

        for line in lines:
            line = line.strip()
            if line.startswith('ì œëª©:'):
                title = line.replace('ì œëª©:', '').strip()
            elif line.startswith('ìš”ì•½:'):
                summary = line.replace('ìš”ì•½:', '').strip()
            elif line.startswith('-') or line.startswith('â€¢'):
                point = line.lstrip('-â€¢').strip()
                if point:
                    points.append(point)

        if not title and lines:
            title = lines[0].replace('ì œëª©:', '').strip()[:20]

        # í¬ì¸íŠ¸ì— ì¹´í…Œê³ ë¦¬ í• ë‹¹
        categorized_items = [
            {"label": infer_category(point), "content": point}
            for point in points[:5]
        ]

        return {
            "time": time_range,
            "title": title,
            "summary": summary,
            "points": points[:5],
            "categorized_items": categorized_items
        }

    def _cluster_into_agendas(self, chunk_summaries: List[Dict]) -> List[Dict]:
        """ì²­í¬ ìš”ì•½ë“¤ì„ ì•ˆê±´ë³„ë¡œ í´ëŸ¬ìŠ¤í„°ë§"""
        all_content = "\n".join([
            f"- {s['title']}: {s['summary']}"
            for s in chunk_summaries
        ])

        prompt = f"""ë‹¤ìŒ íšŒì˜ ë‚´ìš©ë“¤ì„ 3-5ê°œì˜ ì£¼ìš” ì•ˆê±´ìœ¼ë¡œ ê·¸ë£¹í™”í•˜ì„¸ìš”.

{all_content}

ê° ì•ˆê±´ì— ëŒ€í•´:
ì•ˆê±´1: (ì•ˆê±´ ì œëª©)
- ê´€ë ¨ ë‚´ìš© ìš”ì•½

ì•ˆê±´2: (ì•ˆê±´ ì œëª©)
- ê´€ë ¨ ë‚´ìš© ìš”ì•½

í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”:"""

        response = self._call_llm(prompt)

        # íŒŒì‹±í•˜ì—¬ ì•ˆê±´ ëª©ë¡ ìƒì„±
        agendas = []
        current_agenda = None

        for line in response.strip().split('\n'):
            line = line.strip()
            if re.match(r'^ì•ˆê±´\d+:', line):
                if current_agenda:
                    agendas.append(current_agenda)
                title = re.sub(r'^ì•ˆê±´\d+:\s*', '', line).strip()
                current_agenda = {"title": title, "items": []}
            elif line.startswith('-') and current_agenda:
                content = line.lstrip('-').strip()
                if content:
                    current_agenda["items"].append({
                        "label": infer_category(content),
                        "content": content
                    })

        if current_agenda:
            agendas.append(current_agenda)

        # ì•ˆê±´ì´ ì—†ìœ¼ë©´ ì²­í¬ë³„ë¡œ ì•ˆê±´ ìƒì„±
        if not agendas:
            agendas = [
                {"title": chunk["title"], "items": chunk["categorized_items"]}
                for chunk in chunk_summaries
            ]

        return agendas

    def _extract_main_topics(self, chunk_summaries: List[Dict]) -> List[str]:
        """ì£¼ìš” ì£¼ì œ ì¶”ì¶œ"""
        all_content = "\n".join([
            f"{s['title']}: {', '.join(s['points'])}"
            for s in chunk_summaries
        ])

        prompt = f"""ë‹¤ìŒ íšŒì˜ ë‚´ìš©ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ì£¼ìš” ì£¼ì œ 3-5ê°œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

{all_content}

ì£¼ìš” ì£¼ì œë§Œ ê°„ê²°í•˜ê²Œ ë‚˜ì—´í•˜ì„¸ìš” (ê° ì¤„ì— - ë¡œ ì‹œì‘):"""

        response = self._call_llm(prompt)
        return parse_bullet_list(response, max_items=5, max_length=60)

    def _extract_action_items(self, chunk_summaries: List[Dict]) -> List[str]:
        """ë‹¤ìŒ í•  ì¼ ì¶”ì¶œ"""
        all_content = "\n".join([
            f"{s['title']}: {', '.join(s['points'])}"
            for s in chunk_summaries
        ])

        prompt = f"""ë‹¤ìŒ íšŒì˜ ë‚´ìš©ì—ì„œ ì•ìœ¼ë¡œ í•´ì•¼ í•  ì¼(ì•¡ì…˜ ì•„ì´í…œ)ì„ ì¶”ì¶œí•˜ì„¸ìš”.
ê²°ì •ëœ ì‚¬í•­, ëˆ„êµ°ê°€ í•´ì•¼ í•  ì¼, ê²€í† ê°€ í•„ìš”í•œ ì‚¬í•­ ë“±ì„ ì°¾ìœ¼ì„¸ìš”.

{all_content}

ë‹¤ìŒ í•  ì¼ë§Œ ê°„ê²°í•˜ê²Œ ë‚˜ì—´í•˜ì„¸ìš” (ê° ì¤„ì— - ë¡œ ì‹œì‘):"""

        response = self._call_llm(prompt)
        return parse_bullet_list(response, max_items=7, max_length=80)

    def summarize(self, transcript: str, verbose: bool = True) -> HybridSummary:
        """ì „ì‚¬ë³¸ì„ í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ìœ¼ë¡œ ìš”ì•½"""
        start_time = time.time()

        # 1. ì²­í¬ ë¶„í• 
        chunks = chunk_transcript(transcript)
        if verbose:
            print(f"ì²­í¬ ìˆ˜: {len(chunks)}")

        # 2. Map: ê° ì²­í¬ í•˜ì´ë¸Œë¦¬ë“œ ìš”ì•½
        chunk_summaries = []
        for i, (time_range, chunk) in enumerate(chunks):
            if verbose:
                print(f"  [{i+1}/{len(chunks)}] {time_range} ì²˜ë¦¬ ì¤‘...", end=" ")

            chunk_start = time.time()
            summary = self._summarize_chunk(time_range, chunk)
            chunk_summaries.append(summary)

            if verbose:
                print(f"ì™„ë£Œ ({time.time() - chunk_start:.1f}ì´ˆ)")

        # 3. Reduce: ì£¼ìš” ì •ë³´ ì¶”ì¶œ
        if verbose:
            print("\nì£¼ìš” ì£¼ì œ ì¶”ì¶œ ì¤‘...")
        main_topics = self._extract_main_topics(chunk_summaries)

        if verbose:
            print("ë‹¤ìŒ í•  ì¼ ì¶”ì¶œ ì¤‘...")
        action_items = self._extract_action_items(chunk_summaries)

        if verbose:
            print("ì•ˆê±´ë³„ í´ëŸ¬ìŠ¤í„°ë§ ì¤‘...")
        agenda_items = self._cluster_into_agendas(chunk_summaries)

        # 4. íƒ€ì„ë¼ì¸ ìš”ì•½ ì •ë¦¬
        timeline_summaries = [
            {"time": s["time"], "title": s["title"], "points": s["points"]}
            for s in chunk_summaries
        ]

        # 5. ê²°ê³¼ ê²€ì¦
        self._validate_summary(
            main_topics=main_topics,
            action_items=action_items,
            agenda_items=agenda_items,
            chunk_summaries=chunk_summaries,
            verbose=verbose
        )

        # 6. ê²°ê³¼ í¬ë§·íŒ…
        raw_text = self._format_output(
            main_topics, action_items, agenda_items, timeline_summaries
        )

        processing_time = time.time() - start_time
        if verbose:
            print(f"\nì´ ì†Œìš” ì‹œê°„: {processing_time:.1f}ì´ˆ")

        return HybridSummary(
            main_topics=main_topics,
            action_items=action_items,
            timeline_summaries=timeline_summaries,
            agenda_items=agenda_items,
            raw_text=raw_text,
            processing_time=processing_time
        )

    def _validate_summary(
        self,
        main_topics: List[str],
        action_items: List[str],
        agenda_items: List[Dict],
        chunk_summaries: List[Dict],
        verbose: bool = True
    ) -> None:
        """
        ìš”ì•½ ê²°ê³¼ ê²€ì¦

        Raises:
            SummaryValidationError: ê²°ê³¼ê°€ ë¹„ì–´ìˆì„ ë•Œ (strict_validation=True)
        """
        issues = []

        # 1. ì£¼ìš” ì£¼ì œ ì²´í¬
        if not main_topics:
            issues.append("ì£¼ìš” ì£¼ì œê°€ ë¹„ì–´ìˆìŒ")

        # 2. ì•ˆê±´ ì²´í¬
        empty_agendas = sum(1 for a in agenda_items if not a.get('title'))
        if empty_agendas == len(agenda_items) and agenda_items:
            issues.append(f"ëª¨ë“  ì•ˆê±´({len(agenda_items)}ê°œ)ì´ ë¹„ì–´ìˆìŒ")

        # 3. ì²­í¬ ìš”ì•½ ì²´í¬
        empty_chunks = sum(1 for c in chunk_summaries if not c.get('title'))
        if empty_chunks > len(chunk_summaries) * 0.5:
            issues.append(f"ì²­í¬ ìš”ì•½ì˜ {empty_chunks}/{len(chunk_summaries)}ê°œê°€ ë¹„ì–´ìˆìŒ")

        if issues:
            warning_msg = "ìš”ì•½ ê²°ê³¼ ê²€ì¦ ê²½ê³ :\n" + "\n".join(f"  - {i}" for i in issues)

            if verbose:
                print(f"\nâš ï¸ {warning_msg}")

            logger.warning(warning_msg)

            if self.strict_validation:
                raise SummaryValidationError(
                    f"ìš”ì•½ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. LLM ì‘ë‹µì„ í™•ì¸í•˜ì„¸ìš”.\n{warning_msg}"
                )

    def _format_output(
        self,
        topics: List[str],
        actions: List[str],
        agendas: List[Dict],
        timeline: List[Dict]
    ) -> str:
        """í†µí•© ì¶œë ¥ í¬ë§·"""
        lines = []

        # Part 1: ìì—° ìš”ì•½
        lines.append("=" * 50)
        lines.append("ğŸ“‹ ìš”ì•½")
        lines.append("=" * 50)
        lines.append("")
        lines.append("ã€ì£¼ìš” ì£¼ì œã€‘")
        lines.extend(f"â€¢ {topic}" for topic in topics)
        lines.append("")
        lines.append("ã€ë‹¤ìŒ í•  ì¼ã€‘")
        lines.extend(f"â˜ {item}" for item in actions)
        lines.append("")

        # Part 2: êµ¬ì¡°í™” ìš”ì•½
        lines.append("=" * 50)
        lines.append("ğŸ“ ì•ˆê±´ë³„ ìƒì„¸")
        lines.append("=" * 50)
        lines.append("")

        for i, agenda in enumerate(agendas, 1):
            lines.append(f"{i}. {agenda['title']}")
            for item in agenda.get('items', []):
                lines.append(f"   [{item['label']}] {item['content']}")
            lines.append("")

        # Part 3: íƒ€ì„ë¼ì¸
        lines.append("=" * 50)
        lines.append("â±ï¸ íƒ€ì„ë¼ì¸")
        lines.append("=" * 50)
        lines.append("")

        for section in timeline:
            lines.append(f"â–¸ {section['time']}")
            lines.append(f"  {section['title']}")
            for point in section['points'][:3]:
                lines.append(f"    - {point}")
            lines.append("")

        return '\n'.join(lines)

    def to_minutes_json(self, summary: HybridSummary, metadata: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """HybridSummaryë¥¼ íšŒì˜ë¡ JSONìœ¼ë¡œ ë³€í™˜"""
        now = datetime.now()
        meta = metadata or {}

        return {
            "title": "íšŒ  ì˜  ë¡",
            "docNumber": meta.get("docNumber", f"NO {now.year}-{now.month:02d}"),
            "department": meta.get("department", ""),
            "location": meta.get("location", ""),
            "datetime": meta.get("datetime", now.strftime("%Yë…„ %mì›” %dì¼ %Hì‹œ")),
            "organizer": meta.get("organizer", ""),
            "agendaSummary": summary.main_topics,
            "agendaDetails": summary.agenda_items,
            "attendees": meta.get("attendees", ""),
            "attendeeCount": meta.get("attendeeCount", ""),
            "absentees": meta.get("absentees", ""),
            "absenteeCount": meta.get("absenteeCount", "")
        }

    def to_meeting_summary(
        self,
        summary: HybridSummary,
        meeting_id: str,
        model_name: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        HybridSummaryë¥¼ MeetingSummary í˜¸í™˜ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        (main_worker.pyì—ì„œ Supabase ì €ì¥ìš©)
        """
        # ì•ˆê±´ì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
        categories = set()
        for agenda in summary.agenda_items:
            for item in agenda.get('items', []):
                categories.add(item.get('label', ''))

        return {
            "meeting_id": meeting_id,
            "summary": summary.raw_text,
            "key_points": summary.main_topics,
            "action_items": summary.action_items,
            "topics": [agenda['title'] for agenda in summary.agenda_items],
            "categories": list(categories - {''}),  # ë¹ˆ ë¬¸ìì—´ ì œê±°
            "sentiment": None,
            "model_used": model_name or self.model
        }

    def generate_docx(
        self,
        transcript: str,
        output_path: str,
        metadata: Optional[Dict[str, Any]] = None,
        verbose: bool = True
    ) -> str:
        """ì „ì‚¬ë³¸ì—ì„œ DOCX íšŒì˜ë¡ ì§ì ‘ ìƒì„±"""
        summary = self.summarize(transcript, verbose=verbose)
        minutes_json = self.to_minutes_json(summary, metadata)

        # JSON ì €ì¥
        json_path = Path(output_path).with_suffix('.json')
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(minutes_json, f, ensure_ascii=False, indent=2)

        if verbose:
            print(f"\níšŒì˜ë¡ JSON ì €ì¥: {json_path}")

        # DOCX ìƒì„±
        script_path = Path(__file__).parent / "generate_minutes_docx.js"

        try:
            result = subprocess.run(
                ["node", str(script_path), str(json_path), output_path],
                capture_output=True,
                text=True,
                cwd=str(Path(__file__).parent)
            )

            if result.returncode == 0:
                if verbose:
                    print(f"DOCX ìƒì„± ì™„ë£Œ: {output_path}")
                return output_path
            else:
                print(f"DOCX ìƒì„± ì˜¤ë¥˜: {result.stderr}")
                return str(json_path)

        except FileNotFoundError:
            print("Node.jsê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ - JSONë§Œ ìƒì„±ë¨")
            return str(json_path)


def summarize_file(
    input_path: str,
    output_path: Optional[str] = None,
    model: str = DEFAULT_MODEL,
    output_format: str = "text",
    strict: bool = True
) -> None:
    """
    íŒŒì¼ì—ì„œ ì „ì‚¬ë³¸ì„ ì½ì–´ í•˜ì´ë¸Œë¦¬ë“œ ìš”ì•½

    Args:
        input_path: ì…ë ¥ ì „ì‚¬ë³¸ íŒŒì¼ ê²½ë¡œ
        output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (ì—†ìœ¼ë©´ ìë™ ìƒì„±)
        model: Ollama ëª¨ë¸ëª…
        output_format: ì¶œë ¥ í˜•ì‹ (text, json, docx)
        strict: ì—„ê²© ëª¨ë“œ (ë¹ˆ ê²°ê³¼ ì‹œ ì˜ˆì™¸ ë°œìƒ)
    """
    with open(input_path, 'r', encoding='utf-8') as f:
        content = f.read()

    # íƒ€ì„ìŠ¤íƒ¬í”„ ë¼ì¸ ì¶”ì¶œ
    lines = content.strip().split('\n')
    transcript_lines = [
        line.strip() for line in lines
        if line.strip().startswith('[') and 's]' in line
    ]
    transcript = '\n'.join(transcript_lines) if transcript_lines else content

    print(f"ì…ë ¥ íŒŒì¼: {input_path}")
    print(f"ì „ì‚¬ë³¸ ê¸¸ì´: {len(transcript)} ì")
    print(f"ëª¨ë¸: {model}")
    print(f"ì¶œë ¥ í˜•ì‹: {output_format}")
    print(f"ì—„ê²© ëª¨ë“œ: {strict}")
    print("-" * 50)

    try:
        summarizer = HybridSummarizer(
            model=model,
            check_health_on_init=True,
            strict_validation=strict
        )
    except OllamaConnectionError as e:
        print(f"\nâŒ Ollama ì—°ê²° ì‹¤íŒ¨:\n{e}")
        return

    base_path = input_path.rsplit('.', 1)[0]

    try:
        if output_format == "docx":
            output_path = output_path or f"{base_path}_íšŒì˜ë¡.docx"
            summarizer.generate_docx(transcript, output_path)

        elif output_format == "json":
            output_path = output_path or f"{base_path}_íšŒì˜ë¡.json"
            summary = summarizer.summarize(transcript)
            minutes_json = summarizer.to_minutes_json(summary)
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(minutes_json, f, ensure_ascii=False, indent=2)
            print(f"\nâœ… ì €ì¥ ì™„ë£Œ: {output_path}")

        else:
            output_path = output_path or f"{base_path}_í•˜ì´ë¸Œë¦¬ë“œìš”ì•½.txt"
            summary = summarizer.summarize(transcript)
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(summary.raw_text)
            print(f"\nâœ… ì €ì¥ ì™„ë£Œ: {output_path}")
            print("=" * 50)
            print(summary.raw_text)

    except (OllamaConnectionError, OllamaEmptyResponseError) as e:
        print(f"\nâŒ LLM ì˜¤ë¥˜: {e}")
    except SummaryValidationError as e:
        print(f"\nâŒ ìš”ì•½ ê²€ì¦ ì‹¤íŒ¨: {e}")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description="í•˜ì´ë¸Œë¦¬ë“œ íšŒì˜ ìš”ì•½ê¸° (êµ¬ì¡°í™” + ë¹„êµ¬ì¡°í™” í†µí•©)"
    )
    parser.add_argument('input_file', help='ì…ë ¥ ì „ì‚¬ë³¸ íŒŒì¼')
    parser.add_argument('-o', '--output', help='ì¶œë ¥ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('-f', '--format', default='text',
                        choices=['text', 'json', 'docx'],
                        help='ì¶œë ¥ í˜•ì‹')
    parser.add_argument('-m', '--model', default=DEFAULT_MODEL,
                        help='Ollama ëª¨ë¸ëª…')
    parser.add_argument('--no-strict', action='store_true',
                        help='ì—„ê²© ëª¨ë“œ ë¹„í™œì„±í™” (ë¹ˆ ê²°ê³¼ í—ˆìš©)')

    args = parser.parse_args()

    summarize_file(
        args.input_file,
        args.output,
        args.model,
        args.format,
        strict=not args.no_strict
    )
